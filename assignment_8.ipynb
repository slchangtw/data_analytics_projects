{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import graphviz\n",
    "from graphviz import Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read the data set from the zip file and partition it into training and testing data. You should have approximately 75% training data and 25% testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be divided into training and testing data by `train_test_split()`, and the sizes are set according to the task's requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"data/data_X.csv\")\n",
    "y = pd.read_csv(\"data/data_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.75, 0.25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.shape[0] / X.shape[0], X_test.shape[0] / X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.75, 0.25)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_train.shape[0] / y.shape[0], y_test.shape[0] / y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a tree classifier to predict the labels from the data set. Use only the training data you created above. How many leaves are there? What is the depth of the tree? Show the nodes of the resulting tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree model are aimed at finding a criterion to cut feature space into smaller rectangles, thus reach the goal of classification. The criterion can be derived from several ways, and Gini impunity is one of those measures. Gini impunity measures the possibility that an item in a set is mislabeled, and hence the criterion is decided by the point that achieves least Gini impunity. [1][2]\n",
    "\n",
    "`DecisionTreeClassifier` can be used to obtain decision tree models, and the results are shown below. Since the maximum depth was not restricted for the model, we obtained a fully-grown tree, whose leaves (nodes without children) have zero Gini impunity. Also, the depth is 17, the number of leaves is 158 by counting nodes with zero Gini impunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=42, splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/tree.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.tree_.max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(clf.tree_.impurity == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examine the result by reporting the misclassification rate on both the testing and training data set. Would you say this classifier produces a good result? Why? Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix below indicates that training samples are all correctly labeled, an outcome due to the trained classifier is a fully-grown tree, which attains perfect accurary when predicting training set.\n",
    "\n",
    "In fact, however, a fully grown tree comes with high variance, since the effect of an incorrect label will propagate from root to leaves. Therefore the model can be highly sensitive to a slight change in input data, a phenomenon also known as overfitting[2]. The instability can be seen from the confusion matrix with predictions of the test dataset. Each label's misclassification rate except the last one lies between 5% and 10% instead of 0%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 923,    0,    0,    0,    0,    0],\n",
       "       [   0,  817,    0,    0,    0,    0],\n",
       "       [   0,    0,  730,    0,    0,    0],\n",
       "       [   0,    0,    0,  958,    0,    0],\n",
       "       [   0,    0,    0,    0, 1022,    0],\n",
       "       [   0,    0,    0,    0,    0, 1064]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred = clf.predict(X_train)\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_train, y_train_pred)\n",
    "1 - np.diag(cm) / np.sum(cm, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[285,  11,   7,   0,   0,   0],\n",
       "       [ 15, 233,   8,   0,   0,   0],\n",
       "       [ 13,  14, 229,   0,   0,   0],\n",
       "       [  0,   0,   0, 300,  28,   0],\n",
       "       [  1,   0,   0,  26, 325,   0],\n",
       "       [  0,   0,   0,   0,   0, 343]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09235669,  0.09689922,  0.06147541,  0.0797546 ,  0.07932011,  0.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "mis_rates_clf = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "1 - mis_rates_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train a classifier to predict the labels from the data set using the following approaches:\n",
    "\n",
    "1. a bagging approach,\n",
    "2. a boosting approach,\n",
    "3. a random forest approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the high variance in decision tree models could have, a widely used approach is to train several pruned trees and then aggregate the results. Bagging (Bootstrap Aggregating) is basically developed on this notion: the model randomly samples a number of data points, then constructs decision tree using the subsamples, and finally makes predictions based on which label appears the most frequently in those decision trees.[2][3]\n",
    "\n",
    "Bagging can be carried out by `BaggingClassifier`. The results show that though there exists little increase in misclassification rates with training dataset, the misclassification rates with test dataset improve by 1% to 7%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Bagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
       "         n_estimators=10, n_jobs=1, oob_score=False, random_state=42,\n",
       "         verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc = BaggingClassifier(random_state=42)\n",
    "bc.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 923,    0,    0,    0,    0,    0],\n",
       "       [   1,  814,    2,    0,    0,    0],\n",
       "       [   0,    1,  729,    0,    0,    0],\n",
       "       [   0,    0,    0,  957,    1,    0],\n",
       "       [   0,    0,    0,    5, 1017,    0],\n",
       "       [   0,    0,    0,    0,    0, 1064]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred = bc.predict(X_train)\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00108225,  0.00122699,  0.00273598,  0.00519751,  0.00098232,  0.        ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "mis_rates_bc_train = np.diag(cm_train) / np.sum(cm_train, axis = 0)\n",
    "1 - mis_rates_bc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[299,   4,   0,   0,   0,   0],\n",
       "       [  2, 250,   4,   0,   0,   0],\n",
       "       [  5,   8, 243,   0,   0,   0],\n",
       "       [  0,   0,   0, 312,  16,   0],\n",
       "       [  0,   0,   0,  22, 330,   0],\n",
       "       [  0,   0,   0,   0,   0, 343]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = bc.predict(X_test)\n",
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02287582,  0.04580153,  0.01619433,  0.06586826,  0.04624277,  0.        ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "mis_rates_bc_test = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "1 - mis_rates_bc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Boosting (Gradient Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental idea of boosting is to train trees iteratively: a new tree is built on the information of previous trees. The information can be acquired by a loss function, which measures the difference of true values and predicted values. To minimize the loss function, gradient descent is a common approach in pratice. And gradient boosting combines the concepts of boosting and gradient descent to achieve better performance.[2][4]\n",
    "\n",
    "The following results show that the gradient boosting (by `GradientBoostingClassifier`) works successfully both on the training and test datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=42,\n",
       "              subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(random_state=42)\n",
    "gbc.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 923,    0,    0,    0,    0,    0],\n",
       "       [   0,  817,    0,    0,    0,    0],\n",
       "       [   0,    0,  730,    0,    0,    0],\n",
       "       [   0,    0,    0,  958,    0,    0],\n",
       "       [   0,    0,    0,    0, 1022,    0],\n",
       "       [   0,    0,    0,    0,    0, 1064]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred = gbc.predict(X_train)\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "mis_rates_gbc_train = np.diag(cm_train) / np.sum(cm_train, axis = 0)\n",
    "1 - mis_rates_gbc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[302,   1,   0,   0,   0,   0],\n",
       "       [  0, 256,   0,   0,   0,   0],\n",
       "       [  0,   1, 255,   0,   0,   0],\n",
       "       [  0,   0,   0, 315,  13,   0],\n",
       "       [  0,   0,   0,   8, 344,   0],\n",
       "       [  0,   0,   0,   0,   0, 343]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = gbc.predict(X_test)\n",
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.00775194,  0.        ,  0.0247678 ,  0.03641457,  0.        ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "mis_rates_gbc_test = np.diag(cm_test) / np.sum(cm_test, axis = 0)\n",
    "1 - mis_rates_gbc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest can be considered as an augmented model of Bagging. The main difference is that when training grow small trees, this model choose not only subsamples, but also only a subset of the features.[2][5]\n",
    "\n",
    "After applying random forest (by `RandomForestClassifier`), we can see that the misclassification rates with test dataset decrease compared to that of the single decision tree in task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=42,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrc = RandomForestClassifier(random_state=42)\n",
    "rrc.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 923,    0,    0,    0,    0,    0],\n",
       "       [   0,  817,    0,    0,    0,    0],\n",
       "       [   1,    0,  729,    0,    0,    0],\n",
       "       [   0,    0,    0,  958,    0,    0],\n",
       "       [   0,    0,    0,    2, 1020,    0],\n",
       "       [   0,    0,    0,    0,    0, 1064]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred = rrc.predict(X_train)\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00108225,  0.        ,  0.        ,  0.00208333,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "mis_rates_rrc_train = np.diag(cm_train) / np.sum(cm_train, axis = 0)\n",
    "1 - mis_rates_rrc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[300,   2,   1,   0,   0,   0],\n",
       "       [  5, 249,   2,   0,   0,   0],\n",
       "       [  5,   9, 242,   0,   0,   0],\n",
       "       [  0,   0,   0, 314,  14,   0],\n",
       "       [  0,   0,   0,  23, 329,   0],\n",
       "       [  0,   0,   0,   0,   0, 343]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = rrc.predict(X_test)\n",
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03225806,  0.04230769,  0.0122449 ,  0.06824926,  0.04081633,  0.        ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "mis_rates_rrc_test = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "1 - mis_rates_rrc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Examine the result by reporting the misclassification rates of all approaches on both the testing and training data set. Compare the results using the Gini index. Which approach would you recommend and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All misclassification rates have already been listed in task 4, and the Gini index for each approach is computed below. Gradient boosting is recommended For this dataset, given that it outperforms both in training and test datasets with regard to Gini impunity (0 and 0.067). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011186905161037845"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_gini_train = sum(mis_rates_bc_train * (1 - mis_rates_bc_train))\n",
    "bc_gini_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc_gini_train = sum(mis_rates_gbc_train * (1 - mis_rates_gbc_train))\n",
    "gbc_gini_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0031600728704016434"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrc_gini_train = sum(mis_rates_rrc_train * (1 - mis_rates_rrc_train))\n",
    "rrc_gini_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18762235216061415"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_gini_test = sum(mis_rates_bc_test * (1 - mis_rates_bc_test))\n",
    "bc_gini_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.066934748512719522"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc_gini_test = sum(mis_rates_gbc_test * (1 - mis_rates_gbc_test))\n",
    "gbc_gini_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18657184464221849"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrc_gini_test = sum(mis_rates_rrc_test * (1 - mis_rates_rrc_test))\n",
    "rrc_gini_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Breiman Leo, Friedman J. H., Olshen, R. A., and Stone, C. J. (1984). Classification and regression trees. Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software.  \n",
    "[2] Hastie Trevor, Tibshirani Robert, and Friedman J. H. (2001). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. New York: Springer  \n",
    "[3] Breiman Leo (1994). Bagging Predictors. Technical Report No. 421  \n",
    "[4] Friedman J. H (1999). Greedy Function Approximation: A Gradient Boosting Machine.  \n",
    "[5] Ho T.K. (1998). The random subspace method for constructing decision forests.  IEEE Transactions on Pattern Analysis and Machine Intelligence, 8, 832 - 844"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
