---
title: "Assignment 10 by Team 3"
author: "Ashutosh Agarwal, Shun-Lung Chang, Pooja Natu"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", fig.align = 'center', fig.width = 5, fig.height = 3.5)
```

```{r library, include=FALSE}
library(rvest)
library(tidytext)
library(stringr)
library(text2vec)
library(tokenizers)
library(SnowballC)
library(dplyr)
library(tidyr)
library(purrr)
library(ggplot2)
library(wordcloud)
```

```{r obtain data, echo=FALSE}
url_2008 <- "https://www.jacobs-university.de/press-release-archive-2008"

text_2008 <- read_html(url_2008) %>% 
    html_nodes(xpath = "//font[@class='list']") %>% 
    html_text()

# remove meaningless item
text_2008 <- text_2008[-11]

# remove more ... »
text_2008 <- str_replace(text_2008, patter = " more ... »", replacement = "")

# retrieve content in archive 2015
url_2015 <- "https://www.jacobs-university.de/press-release-archive-2015"

text_2015 <- read_html(url_2015) %>% 
    html_nodes(xpath = "//div[@class='news-result clear-block']") %>% 
    html_text()

final_text <- c(text_2008, text_2015)

# extract year
year <- str_match(final_text, pattern = ", ([0-9]{4})[a-zA-Z„(0-9»'ÄÖÜ]?")[, 2]

# remove digits
final_text <- gsub('[0-9]+', '', final_text)

text_df <- data_frame(id = 1:length(final_text),
                      text = final_text,
                      year = year)
```
This study was conducted in R. The source code can be found [here]().

## 1. Convert the html to text files and separate the individual news items. The individual press release items serve as documents.
```{r}
colnames(text_df)
```

```{r}
text_df[2]
```

## 2. Remove stop words and perform stemming.

```{r}
t <- text_df %>%
    unnest_tokens(word, text) %>% 
    anti_join(tibble(word = c(stopwords("de"), stopwords("en")))) %>% 
    mutate(stemmed_word = wordStem(word))

head(t)
```
## 3. Perform a frequency analysis to compute the term-document (TD) matrix. What are the most common terms?

```{r}
top_5_words <- t %>% 
    group_by(stemmed_word) %>% 
    count(sort = TRUE) %>% 
    ungroup() %>% 
    slice(1:5) 
```

```{r, echo=FALSE}
ggplot(top_5_words) + 
    geom_col(aes(x = reorder(stemmed_word, n), y = n)) +
    xlab(NULL) +
    coord_flip() +
    theme_bw()
```

```{r}
word_counts <- t %>% 
    group_by(id, stemmed_word) %>% 
    count() %>% 
    arrange(id, -n) %>% 
    ungroup()

td <- word_counts %>% spread(stemmed_word, n, fill = 0) %>% 
    select(-id) %>% 
    as.matrix()
```

```{r, echo=FALSE}
td[1:10, top_5_words$stemmed_word]
```

## 4. Compute inverse-document frequency (IDF) and term importance (TI). What are now the most common terms?

```{r}
tf_idf <- word_counts %>% 
    bind_tf_idf(term = stemmed_word, document = id, n = n)
```

```{r, echo=FALSE}
tf_idf %>%
    filter(id == 1) %>% 
    arrange(-tf_idf) %>% 
    slice(1:5) %>% 
    ggplot() + 
    geom_col(aes(x = reorder(stemmed_word, tf_idf), y = tf_idf)) +
    xlab(NULL) +
    coord_flip() +
    theme_bw()
```

```{r}
tf_idf %>%
    filter(id == 1) %>% 
    arrange(tf_idf) %>% 
    slice(1:5) %>% 
    ggplot() + 
    geom_col(aes(x = reorder(stemmed_word, -tf_idf), y = tf_idf)) +
    xlab(NULL) +
    coord_flip() +
    theme_bw()
```
## 5. Compute pairwise cosine and Euclidean distance between all documents.

```{r}
cos_dist <- dist2(td, method = 'cosine')
euc_dist <- dist2(td, method = 'euclidean')
```

```{r}
cos_dist[1:3, 1:3]
```

```{r}
euc_dist[1:3, 1:3]
```

## 6. Apply a multi-dimensional scaling approach to the distance matrix and render a 2D scatter- plot. Compare the two distance metrics.

```{r}
scaled_cos_dist <- scale(cos_dist)
scaled_euc_dist <- scale(euc_dist)

pair <- data_frame(cos = scaled_cos_dist[lower.tri(scaled_cos_dist)],
                   euc = scaled_euc_dist[lower.tri(scaled_euc_dist)])
```

```{r, echo=FALSE}
set.seed(42)
sample_pair <- sample_n(pair, 10000)
ggplot(sample_pair) +
    geom_point(aes(x = cos, y = euc)) +
    labs(x = "Cosine Distance", y = "Euclidean Distance") +
    theme_bw()
```

## 7. Capture the year of release during parsing and color code the scatterplot by time. Produce a Word Cloud for each year.

```{r, echo=FALSE}
word_counts_id_year <- t %>% 
    group_by(id, stemmed_word, year) %>% 
    count() %>% 
    arrange(year, id, -n) %>% 
    ungroup()

r <- 1999:2015 %>% 
    map(function(x) {td_x <- word_counts_id_year %>% 
                                filter(year == x) %>% 
                                spread(stemmed_word, n, fill = 0)
                    td_x$id <- NULL
                    td_x$year <- NULL
                    td_x <- as.matrix(td_x)
                    cos_dist <- dist2(td_x, method = 'cosine') %>% scale()
                    euc_dist <- dist2(td_x, method = 'euclidean') %>% scale()
                    data_frame(year = x,
                               cos = cos_dist[lower.tri(cos_dist)],
                               euc = euc_dist[lower.tri(euc_dist)])
                    })
r <- bind_rows(r)
```

```{r, echo=FALSE}
ggplot(r) +
    geom_point(aes(x = cos, y = euc, color = factor(year))) +
    labs(x = "Cosine Distance", y = "Euclidean Distance") +
    theme_bw()
```

```{r}
create_wordcloud <- function(year) {
    d <- t %>% 
        filter(year == year) %>% 
        group_by(stemmed_word) %>% 
        count() %>% 
        ungroup()
    
    wordcloud(words = d$stemmed_word, freq = d$n, 
              max.words = 50, colors = brewer.pal(8, "Dark2"))
} 
```

```{r}
create_wordcloud(2015)
```
