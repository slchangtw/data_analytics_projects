---
title: "Assignment 10 by Team 3"
author: "Ashutosh Agarwal, Shun-Lung Chang, Pooja Natu"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", fig.align = 'center', fig.width = 5, fig.height = 2.5)
```

```{r library, include=FALSE}
library(rvest)
library(tidytext)
library(stringr)
library(text2vec)
library(tokenizers)
library(SnowballC)
library(dplyr)
library(tidyr)
library(purrr)
library(ggplot2)
library(wordcloud)
library(knitr)
```

```{r obtain data, echo=FALSE}
url_2008 <- "https://www.jacobs-university.de/press-release-archive-2008"

text_2008 <- read_html(url_2008) %>% 
    html_nodes(xpath = "//font[@class='list']") %>% 
    html_text()

# remove meaningless item
text_2008 <- text_2008[-11]

# remove more ... »
text_2008 <- str_replace(text_2008, patter = " more ... »", replacement = "")

# retrieve content in archive 2015
url_2015 <- "https://www.jacobs-university.de/press-release-archive-2015"

text_2015 <- read_html(url_2015) %>% 
    html_nodes(xpath = "//div[@class='news-result clear-block']") %>% 
    html_text()

final_text <- c(text_2008, text_2015)

# extract year
year <- str_match(final_text, pattern = ", ([0-9]{4})[a-zA-Z„(0-9»'ÄÖÜ]?")[, 2]

# remove digits
final_text <- gsub('[0-9]+', '', final_text)

text_df <- data_frame(id = 1:length(final_text),
                      text = final_text,
                      year = year)
```
This study was conducted in R. The source code can be found [here](https://github.com/slchangtw/data_analytics_projects/blob/master/assignment_10.Rmd).

## 1. Convert the html to text files and separate the individual news items. The individual press release items serve as documents.
```{r}
colnames(text_df)
```

```{r}
text_df[2] 
```

## 2. Remove stop words and perform stemming.

```{r}
t <- text_df %>%
    unnest_tokens(word, text) %>% 
    anti_join(tibble(word = c(stopwords("de"), stopwords("en")))) %>% 
    mutate(stemmed_word = wordStem(word))
```

```{r, echo=FALSE}
head(t) %>% kable()
```

## 3. Perform a frequency analysis to compute the term-document (TD) matrix. What are the most common terms?

```{r}
top_5_words <- t %>% 
    group_by(stemmed_word) %>% 
    count(sort = TRUE) %>% 
    ungroup() %>% 
    slice(1:5) 
```

```{r, echo=FALSE}
ggplot(top_5_words) + 
    geom_col(aes(x = reorder(stemmed_word, n), y = n)) +
    xlab(NULL) +
    coord_flip() +
    theme_bw()
```

```{r}
word_counts <- t %>% 
    group_by(id, stemmed_word) %>% 
    count() %>% 
    arrange(id, -n) %>% 
    ungroup()

td <- word_counts %>% spread(stemmed_word, n, fill = 0) %>% 
    select(-id) %>% 
    as.matrix()
```

```{r, echo=FALSE}
td[1:10, top_5_words$stemmed_word] %>% kable()
```

## 4. Compute inverse-document frequency (IDF) and term importance (TI). What are now the most common terms?

```{r}
tf_idf <- word_counts %>% 
    bind_tf_idf(term = stemmed_word, document = id, n = n)
```

```{r, echo=FALSE}
head(tf_idf) %>% kable()
```

```{r, echo=FALSE}
tf_idf %>%
    filter(id == 1) %>% 
    arrange(-tf_idf) %>% 
    slice(1:5) %>% 
    ggplot() + 
    geom_col(aes(x = reorder(stemmed_word, tf_idf), y = tf_idf)) +
    xlab(NULL) +
    coord_flip() +
    theme_bw()
```

## 5. Compute pairwise cosine and Euclidean distance between all documents.

```{r}
cos_dist <- dist2(td, method = 'cosine')
euc_dist <- dist2(td, method = 'euclidean')
```

```{r}
cos_dist[1:3, 1:3] %>% kable()
```

```{r}
euc_dist[1:3, 1:3] %>% kable()
```

## 6. Apply a multi-dimensional scaling approach to the distance matrix and render a 2D scatterplot. Compare the two distance metrics.

```{r}
cos_dist_fit <- cmdscale(cos_dist, k = 2)
euc_dist_fit <- cmdscale(euc_dist, k = 2)
```

```{r, echo=FALSE}
ggplot() +
    geom_point(aes(x = cos_dist_fit[, 1], y = cos_dist_fit[, 2])) +
    labs(title = "Scaled Cosine Distance Matrix", x = "x", y = "y")
```


```{r, echo=FALSE}
ggplot() +
    geom_point(aes(x = euc_dist_fit[, 1], y = euc_dist_fit[, 2])) +
    labs(title = "Scaled Euclidean Distance Matrix", x = "x", y = "y")
```

## 7. Capture the year of release during parsing and color code the scatterplot by time. Produce a Word Cloud for each year.

```{r}
create_wordcloud <- function(year) {
    d <- t %>% 
        filter(year == year) %>% 
        group_by(stemmed_word) %>% 
        count() %>% 
        ungroup()
    
    wordcloud(words = d$stemmed_word, freq = d$n, 
              max.words = 50, colors = brewer.pal(8, "Dark2"))
} 
```

```{r, warning=FALSE}
create_wordcloud(2015)
```

```{r, eval=FALSE, include=FALSE}
word_counts_id_year <- t %>% 
    group_by(id, stemmed_word, year) %>% 
    count() %>% 
    arrange(year, id, -n) %>% 
    ungroup()

r <- 2000:2015 %>% 
    map(function(x) {td_x <- word_counts_id_year %>% 
                                filter(year == x) %>% 
                                spread(stemmed_word, n, fill = 0)
                    td_x$id <- NULL
                    td_x$year <- NULL
                    td_x <- as.matrix(td_x)
                    
                    cos_dist <- dist2(td_x, method = 'cosine')
                    euc_dist <- dist2(td_x, method = 'euclidean') 
                    
                    cos_dist_fit <- cmdscale(cos_dist, k = 2)
                    euc_dist_fit <- cmdscale(euc_dist, k = 2)
                    
                    data_frame(cos_x = cos_dist_fit[, 1],
                               cos_y = cos_dist_fit[, 2],
                               euc_x = euc_dist_fit[, 1],
                               euc_y = euc_dist_fit[, 2],
                               year = x
                               )
                    })
r <- bind_rows(r)

ggplot(r) +
    geom_point(aes(x = cos_x, y = cos_y, color = factor(year))) +
    theme_bw()
```
