---
title: "Assignment 10 by Team 3"
author: "Ashutosh Agarwal, Shun-Lung Chang, Pooja Natu"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", fig.align = 'center', fig.width = 5, fig.height = 3)
```

```{r library, include=FALSE}
library(rvest)
library(tidytext)
library(stringr)
library(text2vec)
library(tokenizers)
library(SnowballC)
library(dplyr)
library(tidyr)
library(purrr)
library(ggplot2)
library(wordcloud)
library(knitr)
library(gridExtra)
```

```{r obtain data, echo=FALSE}
url_2008 <- "https://www.jacobs-university.de/press-release-archive-2008"

text_2008 <- read_html(url_2008) %>% 
    html_nodes(xpath = "//font[@class='list']") %>% 
    html_text()

# remove meaningless item
text_2008 <- text_2008[-11]

# remove more ... »
text_2008 <- str_replace(text_2008, patter = " more ... »", replacement = "")

# retrieve content in archive 2015
url_2015 <- "https://www.jacobs-university.de/press-release-archive-2015"

text_2015 <- read_html(url_2015) %>% 
    html_nodes(xpath = "//div[@class='news-result clear-block']") %>% 
    html_text()

final_text <- c(text_2008, text_2015)

# extract year
year <- str_match(final_text, pattern = ", ([0-9]{4})[a-zA-Z„(0-9»'ÄÖÜ]?")[, 2]

# remove digits
final_text <- gsub('[0-9]+', '', final_text)

text_df <- data_frame(id = 1:length(final_text),
                      text = final_text,
                      year = year)
```
This study was conducted in R. The source code can be found [here](https://github.com/slchangtw/data_analytics_projects/blob/master/assignment_10.Rmd).

## 1. Convert the html to text files and separate the individual news items. The individual press release items serve as documents.

The data was retrieved from the Jacobs University press release archives, and then compiled into a dataframe with three variables:

- id: serial number for each press release
- text: title and content of each press release
- year: year of issue

```{r}
colnames(text_df)
```

```{r}
text_df[2] 
```

## 2. Remove stop words and perform stemming.

Stop words can be discarded by `tokenizers::stopwords()`. Stemming can be carried out by `SnowballC::wordStem()`. The results are shown below.

```{r}
t <- text_df %>%
    unnest_tokens(word, text) %>% 
    anti_join(tibble(word = c(stopwords("de"), stopwords("en")))) %>% 
    mutate(stemmed_word = wordStem(word))
```

```{r, echo=FALSE}
head(t) %>% kable()
```

## 3. Perform a frequency analysis to compute the term-document (TD) matrix. What are the most common terms?

After the frequency analysis is performed by `dplyr::count()`, its results indicates that the most frequent three words in these press releases are 'univers', 'jacob', and 'bremen', which come as no surprise.

```{r}
top_5_words <- t %>% 
    group_by(stemmed_word) %>% 
    count(sort = TRUE) %>% 
    ungroup() %>% 
    slice(1:5) 
```

```{r, echo=FALSE}
ggplot(top_5_words) + 
    geom_col(aes(x = reorder(stemmed_word, n), y = n)) +
    xlab(NULL) +
    coord_flip() +
    theme_bw()
```

Furthermore, the term-document matrix below shows that in the first ten press releases, the three most common terms did appear several times.

```{r}
word_counts <- t %>% 
    group_by(id, stemmed_word) %>% 
    count() %>% 
    arrange(id, -n) %>% 
    ungroup()

td <- word_counts %>% spread(stemmed_word, n, fill = 0) %>% 
    select(-id) %>% 
    as.matrix()
```

```{r, echo=FALSE}
td[1:10, top_5_words$stemmed_word] %>% kable(row.names = TRUE)
```

## 4. Compute inverse-document frequency (IDF) and term importance (TI). What are now the most common terms?

Inverse-document frequency and term importance in each press release can be acquired by `tidytext::bind_tf_idf()`. The following table shows that 'univers' gained low importance that resulted from its ubiquitousness in the whole corpus.

```{r}
tf_idf <- word_counts %>% 
    bind_tf_idf(term = stemmed_word, document = id, n = n)
```

```{r, echo=FALSE}
head(tf_idf) %>% kable()
```

A closer look at the term importance of the first document reveals that important terms are surrounding the topics of cyber and emotion. And if people check the press title, which is "Wissenschaft jenseits von Science Fiction: Jacobs University beteiligt sich am CyberEmotions-Project der EU", they can immediately understand the reason behind the results. 

```{r, echo=FALSE}
tf_idf %>%
    filter(id == 1) %>% 
    arrange(-tf_idf) %>% 
    slice(1:5) %>% 
    ggplot() + 
    geom_col(aes(x = reorder(stemmed_word, tf_idf), y = tf_idf)) +
    xlab(NULL) +
    coord_flip() +
    theme_bw()
```

## 5. Compute pairwise cosine and Euclidean distance between all documents.

Applying `text2vec::dist2` on the term-document matrix can assist us in obtaining pairwise similarities (measured by cosine or euclidean distance) of two documents.

```{r}
cos_dist <- dist2(td, method = 'cosine')
euc_dist <- dist2(td, method = 'euclidean')
```

The following are cosine distance matrix and euclidean distance matrix, respectively. Both of them show the first 3 x 3 submatrix. The diagonal elements are all 0 since they are comparisons of identical documents. And lower off-diagonal values suggest more similar documents.

```{r}
cos_dist[1:3, 1:3] %>% kable()
```

```{r}
euc_dist[1:3, 1:3] %>% kable()
```

## 6. Apply a multi-dimensional scaling approach to the distance matrix and render a 2D scatterplot. Compare the two distance metrics.

A multi-dimensional scaling approach can transform a high-dimentional matrix into low-dimentional space. To approach this task, we applied the classical multi-dimensional scaling (by `cmdscale()`) on the two matrices obtained in task 5.

```{r}
cos_dist_fit <- cmdscale(cos_dist, k = 2)
euc_dist_fit <- cmdscale(euc_dist, k = 2)
```

As can be seen from the scatterplots below, a cluster appear in the left-upper quadrant of both plots. Given that most topics of the press were unrelated, those dissimilar pairs in turn formed the cluster in the two plots.

```{r, echo=FALSE}
scaled_cos_dist <- ggplot() +
    geom_point(aes(x = cos_dist_fit[, 1], y = cos_dist_fit[, 2])) +
    labs(title = "Cosine", x = "Coordinate 1", y = "Coordinate 2")

scaled_euc_dist <- ggplot() +
    geom_point(aes(x = euc_dist_fit[, 1], y = euc_dist_fit[, 2])) +
    labs(title = "Euclidean", x = "Coordinate 1", y = "Coordinate 2")

grid.arrange(scaled_cos_dist, scaled_euc_dist, ncol = 2)
```
## 7. Capture the year of release during parsing and color code the scatterplot by time. Produce a Word Cloud for each year.


```{r}
create_wordcloud <- function(year) {
    d <- t %>% 
        filter(year == year) %>% 
        group_by(stemmed_word) %>% 
        count() %>% 
        ungroup()
    
    wordcloud(words = d$stemmed_word, freq = d$n, 
              max.words = 50, colors = brewer.pal(8, "Dark2"))
    text(x = 0.5, y = 1, cex = 0.5, as.character(year))
} 
```

```{r, warning=FALSE, fig.height=2, fig.width=2}
for (i in seq(2015, 2013, -1)) {
    create_wordcloud(i)
}
```

```{r, eval=FALSE, include=FALSE}
word_counts_id_year <- t %>% 
    group_by(id, stemmed_word, year) %>% 
    count() %>% 
    arrange(year, id, -n) %>% 
    ungroup()

r <- 2000:2015 %>% 
    map(function(x) {td_x <- word_counts_id_year %>% 
                                filter(year == x) %>% 
                                spread(stemmed_word, n, fill = 0)
                    td_x$id <- NULL
                    td_x$year <- NULL
                    td_x <- as.matrix(td_x)
                    
                    cos_dist <- dist2(td_x, method = 'cosine')
                    euc_dist <- dist2(td_x, method = 'euclidean') 
                    
                    cos_dist_fit <- cmdscale(cos_dist, k = 2)
                    euc_dist_fit <- cmdscale(euc_dist, k = 2)
                    
                    data_frame(cos_x = cos_dist_fit[, 1],
                               cos_y = cos_dist_fit[, 2],
                               euc_x = euc_dist_fit[, 1],
                               euc_y = euc_dist_fit[, 2],
                               year = x
                               )
                    })
r <- bind_rows(r)

ggplot(r) +
    geom_point(aes(x = cos_x, y = cos_y, color = factor(year))) +
    theme_bw()
```
```{r, eval=FALSE, include=FALSE}
ggplot(r) +
    geom_point(aes(x = euc_x, y = euc_y, color = factor(year))) +
    theme_bw()
```