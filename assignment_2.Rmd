---
title: "Assignment 2 By Team 4"
author: "Salma Bouzid, Shun-Lung Chang, Savitha Singh"
output: 
    pdf_document:
        fig_caption: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", warning = FALSE, message = FALSE)
```

```{r import pachages, include=FALSE}
library(magrittr)
library(ggplot2)
library(data.table)
library(dplyr)
library(tidyr)
library(stringr)
library(lubridate)
library(pryr)
```

# Import data

&nbsp;&nbsp;We first downloaded the datasets from [__here__](https://www.kaggle.com/c/zillow-prize-1/data), then imported them into R as dataframes (**properties** and **transaction** for "properties_2016.csv" and "train_2016.csv" respectively). In addition, the two dataframes were joined as a new dataframe, **joined_df**.

```{r import data, cache=TRUE, results='hide'}
properties <- fread("./data/properties_2016.csv")
transaction <- fread("./data/train_2016.csv")
joined_df <- merge(transaction, properties, all.x = TRUE)
```

# 1. Explain why it is or why it is not a big data problem.

&nbsp;&nbsp;We can safely conclude that analyzing this dataset is not a big data problem, since it fails to satisfy the volume, variety, velocity. It only satisfies the veracity criterion.
 
A closer look at the 4 Vs of big data[2] will enable us to better understand this problem: 

1. The dataset is not voluminous  
After using `pryr::object_size(properties)` in R, we know that properties's memory is merely `r round(object_size(properties) / 1e9, 3)` GBs. According to the table below[1], we know that handling this data set can be done on a consumer PC and does not require extra cores or machines.

![](imgs/table1.png)

2. The dataset is structured     
The dataset is well-defined in labeled rows and columns. In fact, the dataset comes with a dictionary that clearly explains the 58 attributes in the properties data.

3. The dataset is static  
Most of the datasets’ attributes come from government agencies that publish yearly or bi-monthly statistics. Moreover, this dataset does not include the newly user generated input, although users can update their housing information on Zillow’s portal anytime to reflect changes in their home characteristics[3].  

4. We can trust the data  
The dataset’s observations come from public records regarding location and property characteristics. Although it can be missing or outdated[3], we can safely assume that this dataset has not been manipulated to reflect bias that favors one housing area or similar fraudulent behaviors. Therefore, the uncertainty of this problem does not achive the level of big data problems.

# 2. Why is it an analytics problem?

&nbsp;&nbsp;Analytics aims to derive actionable insights from data. To address an analytics issue, we define the problem. Second, statistical models and computing algorithms are used to solve the issue[4]. We will rely on this definition to answer this question.

1. Problem definition  
Buyers and sellers are not equally informed about the value of houses. In fact, some players, such as real estate agents, have information advantage. They are more informed about future gentrification and demographic patterns that impact future house prices[5]. 

2. How does the Zillow fight information asymmetry through data analytics?    
The Zestimate - Zillow’s star index estimates future house prices by analyzing user inputted data and public records. The target feature in this challenge aims to measure the log difference between Zillow’s proprietary estimates and the actual prices in the house market[6]. 

Zillow launched this challenge in order to improve its housing valuation algorithm by learning from the best performing models submitted by Kaggle users.

# 3. How many data attributes are there?

&nbsp;&nbsp;After merging the two datasets we obtain 60 attributes in total. The initial properties dataframe contained 58 attributes while the transaction dataframe contained 3 attributes including the transaction date and target variable.

```{r}
colnames(joined_df)
```

# 4. Identify the type of the 15 attributes you find most relevant in this context

&nbsp;&nbsp;The goal of this data challenge is to predict the *log(error)* value. To select 15 relevant attributes out of 60, we assume that "relevant" means "linearly correlated". Hence, we choose the first 15 attributes with highest absolute correlation coefficients between logerror below.  

&nbsp;&nbsp;To do so, we pick numeric features, use `Hmisc::rcorr()` to get the correlation coefficient matrix, sort the correlation coefficients between logerror decreasingly, and choose the first 15 items. The below barplot showcases the final result. 

```{r, warning=FALSE}
numeric_df <- joined_df[, sapply(joined_df, is.numeric), with = FALSE]

corr_mat <- Hmisc::rcorr(as.matrix(numeric_df))

top15 <- abs(corr_mat$r[2, ]) %>% 
    sort(decreasing = TRUE) %>% 
    .[2:16] %>% 
    names()
```

```{r, echo=FALSE, fig.height=4}
top15_values <- corr_mat$r[2, top15] %>% 
    as.data.frame()
top15_values$features <- rownames(top15_values)
colnames(top15_values) <- c("values", "features")

top15_values %>% 
    ggplot(aes(x = reorder(features, values), y = values)) + 
        geom_bar(stat = "identity", fill = "red") +
        coord_flip() +
        labs(x = "Features", y = "Correlation Coefficient")
```

# 5. Determine whether the task refers to a supervised or unsupervised learning problem

&nbsp;&nbsp;The Zillow home valuation challenge is a supervised learning problem. The target variable namely the *log(error)* is labeled. Hence, we rely on the independent variables to predict a known target.

# 6. Find out what the standard analysis algorithms are for this analytics problem

&nbsp;&nbsp;The Zillow Home Value Kaggle challenge is a **supervised learning** task that deals with a regression problem since the output value is a **real number**. Hence, the standard algorithms used in this task are regression models. 

# 7. Download the data and provide descriptive summaries of the sample data

&nbsp;&nbsp;To summarize the data, we visually display the target variable between January and October 2016, then use the `summary()` function in R to get the minimum, maximum, 1st and 3rd quartile, mean and median and number of missing values for each independent variable. 

```{r, echo = FALSE, fig.height=3, fig.width=4, fig.align='center'}
transaction %>% 
    mutate(year_month = make_date(year = year(transactiondate), month = month(transactiondate))) %>% 
    group_by(year_month) %>% 
    summarise(mean_logerror = mean(logerror)) %>% 
    ggplot(aes(x = year_month, y = mean_logerror)) +
    geom_line(color = "red") + 
    labs(x = "Year-Month", y = "Mean Logerror")
```

```{r}
summary(properties)
```

# 8. Check for completeness of the data! Are there any missings? How are the missings distributed?

&nbsp;&nbsp;After merging the two datasets, we obtain 90275 observations of which 37.1% are missing. 18 features lack 95% of values. Four features namely 'basementsqft', 'buildingclasstypeid', 'finishedsquarefeet13', 'storytypeid' miss 99.99 % of observations.

&nbsp;&nbsp;However, 13 features have 0 missing values such as geographical and house room attributes. 
The below horizontal histogram illustrates the distribution of NaN values across the dataset. 

*Note: It is worth noting that this analysis is incomplete since it only checks for missing values. More missing data can be present in the dataset under other formats.*

```{r, echo=FALSE}
joined_df %>% 
    summarise_all(funs(sum(is.na(.)) / n())) %>% 
    gather(key = "attribute", value = "na_counts") %>% 
    ggplot(aes(x = reorder(attribute, na_counts), y = na_counts)) +
        geom_bar(stat = "identity", fill = "red") +
        coord_flip() + 
        theme(axis.text = element_text(size = 6)) + 
        labs(x = "Feauture", y = "Proportion of Missing Values") 
```

# References

[1] Adalbert F.X. Wilhelm (2017),  The Big Data Challenge: Topics, Applications, Perspectives [Powerpoint slides]

[2] Shafer, T. (2017). The 4 V's of Big Data and Data Science. Elderresearch.com. Retrieved 30 September 2017, from https://www.elderresearch.com/company/blog/42-v-of-big-data

[3] Zillow, I. (2017). What is a Zestimate? Zillow's Zestimate Accuracy | Zillow. Zillow. Retrieved 30 September 2017, from https://www.zillow.com/zestimate/#acc

[4] Cooper, A. (2012). What is Analytics? Definition and Essential Characteristics. cetis.org. Retrieved 30 September 2017, from http://publications.cetis.org.uk/wp-content/uploads/2012/11/What-is-Analytics-Vol1-No-5.pdf

[5] Kurlat, P., & Stroebel, J. (2014). TESTING FOR INFORMATION ASYMMETRIES IN REAL ESTATE MARKETS. http://www.nber.org/. Retrieved 30 September 2017, from http://www.nber.org/papers/w19875.pdf

[6] Zillow Prize: Zillow’s Home Value Prediction (Zestimate) | Kaggle. (2016). Kaggle.com. Retrieved 30 September 2017, from https://www.kaggle.com/c/zillow-prize-1#description

[7] Supervised Learning Workflow and Algorithms - MATLAB & Simulink - MathWorks United Kingdom. (2017). De.mathworks.com. Retrieved 30 September 2017, from https://de.mathworks.com/help/stats/supervised-learning-machine-learning-workflow-and-algorithms.html#buxe4f_
