---
title: "Assignment 2 By Team 4"
author: "Salma Bouzid, Shun-Lung Chang, Savitha Singh"
output: 
    pdf_document:
        fig_caption: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "", warning = FALSE, message = FALSE)
```

```{r import pachages, include=FALSE}
library(magrittr)
library(ggplot2)
library(data.table)
library(dplyr)
library(tidyr)
library(stringr)
library(lubridate)
library(pryr)
```

# Import data

&nbsp;&nbsp;We first downloaded the data sets from [__here__](https://www.kaggle.com/c/zillow-prize-1/data), and then we imported and stored them as R dataframes (**properties** and **transaction** for "properties_2016.csv" and "train_2016.csv" respectively). In addition, the two dataframes were joined as a new dataframe, **joined_df**.

```{r import data, cache=TRUE, results='hide'}
properties <- fread("./data/properties_2016.csv")
transaction <- fread("./data/train_2016.csv")
joined_df <- merge(transaction, properties, all.x = TRUE)
```

# 1. Explain why it is or why it is not a big data problem.

&nbsp;&nbsp;We can safely conclude that analyzing this data set is not a big data problem, since it fails to satisfy the volume, variety, velocity, veracity argument  (Shafer, 2017). 
 
A closer look at the 4Vs of big data will enable us to better understand this problem: 

1. The dataset is not voluminous  
After using `pryr::object_size(properties)` in R, we know that properties's memory is merely `r round(object_size(properties) / 1e9, 3)` GBs. According to the table below^[1]^, we know that handling this dataset can be done on a consumer PC and does not require extra cores or machines.

![](imgs/table1.png)

2. The dataset is structured     
The dataset is well-defined in labeled rows and columns. In fact, the dataset comes with a dictionary that clearly explains the 58 attributes in the properties data.

3. The dataset is static  
Most of the datasets’ attributes come from government agencies that publish yearly or bi-monthly statistics. Moreover, this dataset does not include the newly user generated input, although users can update their housing information on Zillow’s portal anytime to replects changes of their households (Zillow, 2017).

4. We can trust the data  
The dataset’s observations come from public records regarding location and property characteristics. Although it can be missing or outdated (Zillow, 2017), we can safely assume that this dataset has not been manipulated to reflect bias that favors one housing area or any such fraudulent behaviors. Therefore, the uncertainty of this problem does not achive the level of big data problems.

# 2. Why is it an analytics problem?

&nbsp;&nbsp;Analytics aims to derive actionable insights from data. One starts by defining the problem at hand. Second, statistical models and computing algorithms are used to solve the issue. (Cooper, 2012). We will rely on this definition to answer this question.

1. Problem definition
Buyers and sellers are not equally informed about the value of houses. In fact, some players, such as real estate agents, have information advantage. They are more informed about future gentrification and demographic patterns that impact future house prices. (Kurlat and Stroebel, 2014). In order to resolve the information asymmetry in the market, Zillow strives to help house buyers and provide them with precise information of the housing market. (Zillow, 2017)

2. How does the Zillow fight information asymmetry through data analytics?  
By analyzing user input data and public records, Zillow predicts the house values, which is called Zestimate. And
Zillow evaluates the log difference between Zestimate and actual prices to offer house buyers a clearer picture of future house market. Moreover, during the analytics process, Zillow relied on statistical models for precise prediction, and launched this challenge to improve its housing valuation algorithm by learning from the most performing models submitted by Kaggle users

# 3. How many data attributes are there?

&nbsp;&nbsp;As can be seen from the result of `colnames(joined_df)`, the whole data set contains `r ncol(joined_df)` attributes. Also, properties contains `r ncol(properties)` attributes and transaction contains `r ncol(transaction)` attributes.

```{r}
colnames(joined_df)
```

# 4. Identify the type of the 15 attributes you find most relevant in this context

&nbsp;&nbsp;Since the goal of this problem is to find minimal *log(error)*, and we assume that "relevant" means "linearly correlated". We show the first 15 attributes that has highest absolute correlation coefficients between logerror below.  

&nbsp;&nbsp;Firstly, we pickced those features that are numeric, and then used `Hmisc::rcorr()` to get the correlation coefficient matrix. At last we sorted the correlation coefficients between logerror decreasingly, and chose the first 15 items. The result is shown as the barplot.

```{r, warning=FALSE}
numeric_df <- joined_df[, sapply(joined_df, is.numeric), with = FALSE]

corr_mat <- Hmisc::rcorr(as.matrix(numeric_df))

top15 <- abs(corr_mat$r[2, ]) %>% 
    sort(decreasing = TRUE) %>% 
    .[2:16] %>% 
    names()
```

```{r, echo=FALSE, fig.height=4}
top15_values <- corr_mat$r[2, top15] %>% 
    as.data.frame()
top15_values$features <- rownames(top15_values)
colnames(top15_values) <- c("values", "features")

top15_values %>% 
    ggplot(aes(x = reorder(features, values), y = values)) + 
        geom_bar(stat = "identity", fill = "red") +
        coord_flip() +
        labs(x = "Features", y = "Correlation Coefficient")
```

# 5. Determine whether the task refers to a supervised or unsupervised learning problem

&nbsp;&nbsp;This should be a supervised learning task. As noted by Mehryar Mohri et al, the goal of supervised learning proplem is to use labeled data for prediction, namely, there is a dependant variable, which is usually denoted as y, and one uses other independant variables, which is also known as feature, to predict this dependant variable. Given that we have to predict *log(error)* variable using features in properties, so *log(error)* is the labeled variable we need to prediction in a supervised problem.  

# 6. Find out what the standard analysis algorithms are for this analytics problem

&nbsp;&nbsp;In order to find the optimal logerror, a standard analysis algorithm that can be used is the regression analysis. Regression analysis is for estimation the relationship among variables. for this task, one can build regression models to find the relationship between logerror and other variables.

# 7. Download the data and provide descriptive summaries of the sample data

&nbsp;&nbsp;First, we summarised transaction by ploting its trend. One can see an uprising trend of the average logerror of each month.

```{r, echo = FALSE, fig.height=3, fig.width=4, fig.align='center'}
transaction %>% 
    mutate(year_month = make_date(year = year(transactiondate), month = month(transactiondate))) %>% 
    group_by(year_month) %>% 
    summarise(mean_logerror = mean(logerror)) %>% 
    ggplot(aes(x = year_month, y = mean_logerror)) +
    geom_line(color = "red") + 
    labs(x = "Year-Month", y = "Mean Logerror")
```

&nbsp;&nbsp;Also, we used `summary()` in R to summarise properties. One can see a noticeable number of missing values (NAs) in many of the features, and we would carry out a deeper analysis of these missing values in the next question.

```{r}
summary(properties)
```

# 8. Check for completeness of the data! Are there any missings? How are the missings distributed?

&nbsp;&nbsp;For transaction, we first use `complete.cases()` to check whether each observation is complete or not. The result is shown below, and there is no incomplete row in transaction.

```{r}
sum(!complete.cases(transaction))
```

&nbsp;&nbsp;For properties, as can be seen from question 7, there exist a great amount of missing values in many features of  The plot below shows the counts of missing values of each feature and how they distribute. One can see that 18 features lack 95% of values. For variable 'basementsqft', 'buildingclasstypeid', 'finishedsquarefeet13', 'storytypeid', they miss more that 99% of observations. Thirteen features, however, have zero missing values, such as geographical and house room attributes. 

```{r, echo=FALSE}
properties %>% 
    summarise_all(funs(sum(is.na(.)))) %>% 
    gather(key = "attribute", value = "na_counts") %>% 
    ggplot(aes(x = reorder(attribute, na_counts), y = na_counts)) +
        geom_bar(stat = "identity", fill = "red") +
        coord_flip() + 
        theme(axis.text = element_text(size = 6)) + 
        labs(x = "Feauture", y = "Proportion of Missing Values") 
```

# References

[1] Adalbert F.X. Wilhelm (2017),  The Big Data Challenge: Topics, Applications, Perspectives [Powerpoint slides]


